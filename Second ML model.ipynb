{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrBQNFgzq3SJq399ln8V0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greapis/jaeha-shim/blob/main/Second%20ML%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "JhYU3y0ekLa4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IShAch5kVXq",
        "outputId": "b1dc1132-d03d-4401-9ec9-ca6568f7ae91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We will train a neural network to recognize items of clothing from a common dataset called Fashion MNIST.\n",
        "  # It contains 70,000 items of clothing in 10 different categories. Each item of clothing is in a 28x28 greyscale image\n",
        "\n",
        "  #  The Fashion MNIST data is available directly in the tf.keras datasets API. You load it like this:\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist"
      ],
      "metadata": {
        "id": "M4WnuVL3kbzk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling load_data on this object will give you two sets of two lists,\n",
        "# these will be the training and testing values for the graphics that contain the clothing items and their labels.\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "G4w499FOenPm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print a training image, and a training label to see...Experiment with different indices in the array.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(training_images[0])\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MlJFzB7EpNqc",
        "outputId": "3fe04ca0-4912-471e-8686-923eab6f5428"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0\n",
            "    0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62\n",
            "   54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134\n",
            "  144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178\n",
            "  107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216\n",
            "  216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229\n",
            "  223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228\n",
            "  235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198\n",
            "  180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192\n",
            "  169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203\n",
            "  198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240\n",
            "  232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219\n",
            "  222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208\n",
            "  211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230\n",
            "  224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223\n",
            "  255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221\n",
            "  188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117\n",
            "  168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245\n",
            "  239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216\n",
            "  199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191\n",
            "  195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209\n",
            "  210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179\n",
            "  182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll notice that all of the values in the number are between 0 and 255\n",
        "#  If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1, a process called 'normalizing'\n",
        "\n",
        "# Fortunately in Python it's easy to normalize a list like this without looping. You do it like this:\n",
        "\n",
        "training_images  = training_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "DweZxZ22pOFm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now design the model.\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "# Sequential: That defines a SEQUENCE of layers in the neural network\n",
        "\n",
        "# Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
        "\n",
        "# Dense: Adds a layer of neurons\n",
        "\n",
        "# Each layer of neurons need an activation function to tell them what to do. There's lots of options, but just use these for now.\n",
        "\n",
        "# Relu effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
        "\n",
        "# Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!"
      ],
      "metadata": {
        "id": "5tsCBci6peH4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGx0WC9cRQBD",
        "outputId": "a039f83a-0e9d-460c-b023-29d5c0897490"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4969 - accuracy: 0.8266\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3746 - accuracy: 0.8644\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3373 - accuracy: 0.8771\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3137 - accuracy: 0.8853\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2975 - accuracy: 0.8896\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2806 - accuracy: 0.8956\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2677 - accuracy: 0.9009\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2588 - accuracy: 0.9039\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2490 - accuracy: 0.9074\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2399 - accuracy: 0.9100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ee0f7a4640>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we have the test images. We can call model.evaluate, and pass in the two sets,\n",
        "# and it will report back the loss for each. Let's give it a try:\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qpj6YPUugWY",
        "outputId": "54bc2e9e-b7a0-4a95-e60c-eca2d9537f4a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3548 - accuracy: 0.8791\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35476216673851013, 0.8791000247001648]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1\n",
        "\n",
        "# It creates a set of classifications for each of the test images, and\n",
        "# then prints the first entry in the classifications.\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOOSSmd_633F",
        "outputId": "63dc5407-1b1d-4ade-df61-7ee6d7c5b1c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 1ms/step\n",
            "[4.6946735e-09 1.1685120e-09 6.3832362e-10 2.4805683e-12 7.0851498e-11\n",
            " 7.1860472e-05 6.5288987e-08 3.3854740e-03 1.6893034e-06 9.9654096e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5fFTuiw7Lrd",
        "outputId": "c873379c-281c-4cb3-a565-86a1ceb81db3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2:\n",
        "# Let's now look at the layers in your model.\n",
        "# Experiment with different values for the dense layer with 512 neurons.\n",
        "# What different results do you get for loss, training time etc? Why do you think that's the case?p\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x95Slkuc7uMR",
        "outputId": "ed1df687-eea3-47e0-accd-2aa2738357ae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.4712\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3592\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.3233\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.2982\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.2784\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3556\n",
            "313/313 [==============================] - 1s 3ms/step\n",
            "[7.9366237e-06 3.5995026e-08 1.2589182e-08 1.8963739e-08 2.3963224e-08\n",
            " 1.7114580e-02 1.2675077e-06 8.8120617e-02 9.7955735e-06 8.9474565e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3:\n",
        "# What would happen if you remove the Flatten() layer. Why do you think that's the case?\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "\n",
        "# This version has the 'flatten' removed. Replace the above with this one to see the error.\n",
        "#model = tf.keras.models.Sequential([tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "#                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])\n"
      ],
      "metadata": {
        "id": "uqZKICLlPCaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9a2e38-4bd3-4743-e201-4c387317b3c3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5223\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3939\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3537\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3301\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3129\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3832\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "[1.0531872e-06 5.4142490e-09 3.0437634e-06 2.2362062e-07 4.3454529e-06\n",
            " 2.0305289e-02 9.9292492e-06 5.2204547e-03 7.1631592e-05 9.7438389e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4:\n",
        "# Consider the final (output) layers. Why are there 10 of them?\n",
        "# What would happen if you had a different amount than 10? For example, try training the network with 5\n",
        "\n",
        "# You get an error as soon as it finds an unexpected value.\n",
        "# Another rule of thumb -- the number of neurons in the last layer should match the number of classes you are classifying for.\n",
        "# In this case it's the digits 0-9, so there are 10 of them, hence you should have 10 neurons in your final layer.\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "\n",
        "# Replace the above model definiton with this one to see the network with 5 output layers\n",
        "# And you'll see errors as a result!\n",
        "# model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "#                                    tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
        "#                                    tf.keras.layers.Dense(5, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])\n"
      ],
      "metadata": {
        "id": "ADJRctFrNd-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea468b46-7738-471b-d798-0c82adc0b93e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5207\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3952\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3547\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3283\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3114\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3600\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "[1.6466814e-05 1.9623918e-08 4.3432917e-07 4.0068181e-07 4.5800822e-05\n",
            " 9.8027252e-02 4.6429886e-05 6.5194353e-02 7.3568095e-05 8.3659530e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5:\n",
        "# Consider the effects of additional layers in the network.\n",
        "# What will happen if you add another layer between the one with 512 and the final layer with 10.\n",
        "\n",
        "# Ans: There isn't a significant impact --\n",
        "# because this is relatively simple data.\n",
        "# For far more complex data (including color images to be classified as flowers that you'll see in the next lesson),\n",
        "# extra layers are often necessary.\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "xPSDF3-k2nN2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca4bb8b6-776d-45a2-ff94-22275ed5dd5d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 15s 8ms/step - loss: 0.4653\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3520\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3180\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2940\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2787\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3811\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "[1.3312468e-11 1.6758581e-11 1.6410331e-10 4.9044740e-10 3.4204153e-10\n",
            " 3.2302262e-06 1.2333939e-09 2.0886443e-03 2.3748581e-10 9.9790812e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6:\n",
        "# Consider the impact of training for more or less epochs. Why do you think that would be the case?\n",
        "\n",
        "# Try 15 epochs -- you'll probably get a model with a much better loss than the one with 5 Try 30 epochs\n",
        "# you might see the loss value stops decreasing, and sometimes increases.\n",
        "# This is a side effect of something called 'overfitting' which you can learn about [somewhere] and\n",
        "# it's something you need to keep an eye out for when training neural networks.\n",
        "# There's no point in wasting your time training if you aren't improving your loss, right! :)\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, trainig_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=30)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[34])\n",
        "print(test_labels[34])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz8P6UeDltgU",
        "outputId": "e27ecafa-97aa-4d85-da2b-c5e557241abe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5012\n",
            "Epoch 2/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3765\n",
            "Epoch 3/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3412\n",
            "Epoch 4/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3151\n",
            "Epoch 5/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2983\n",
            "Epoch 6/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2818\n",
            "Epoch 7/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2720\n",
            "Epoch 8/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2590\n",
            "Epoch 9/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2476\n",
            "Epoch 10/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2400\n",
            "Epoch 11/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2326\n",
            "Epoch 12/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2230\n",
            "Epoch 13/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2187\n",
            "Epoch 14/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2126\n",
            "Epoch 15/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2047\n",
            "Epoch 16/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1996\n",
            "Epoch 17/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1936\n",
            "Epoch 18/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1872\n",
            "Epoch 19/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1846\n",
            "Epoch 20/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1791\n",
            "Epoch 21/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1753\n",
            "Epoch 22/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1698\n",
            "Epoch 23/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1662\n",
            "Epoch 24/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1620\n",
            "Epoch 25/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1590\n",
            "Epoch 26/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1555\n",
            "Epoch 27/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1534\n",
            "Epoch 28/30\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1486\n",
            "Epoch 29/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1465\n",
            "Epoch 30/30\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1458\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3989\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "[2.3836255e-28 4.1399909e-35 4.0107244e-25 5.3154075e-35 7.0962625e-22\n",
            " 2.0680717e-21 2.8668589e-26 7.6218411e-36 9.9999994e-01 0.0000000e+00]\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7:\n",
        "# Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1.\n",
        "# What would be the impact of removing that? Here's the complete code to give it a try.\n",
        "# Why do you think you get different results?\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# To experiment with removing normalization, comment out the following 2 lines\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1DQhqW3zi9M",
        "outputId": "fc7a2afe-4cec-427e-8a11-238ac3298930"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.4721\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3615\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3221\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2964\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2798\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3460\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "[6.5043508e-07 4.6193685e-10 1.8106814e-08 3.6484407e-10 1.5028701e-07\n",
            " 7.8341377e-04 2.4294252e-07 1.2003119e-02 1.5524978e-06 9.8721087e-01]\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 8:\n",
        "# Earlier when you trained for extra epochs you had an issue where your loss might change.\n",
        "# It might have taken a bit of time for you to wait for the training to do that, and\n",
        "# you might have thought 'wouldn't it be nice if I could stop the training when I reach a desired value?'\n",
        "# -- i.e. 95% accuracy might be enough for you, and\n",
        "# if you reach that after 3 epochs, why sit around waiting for it to finish a lot more epochs....\n",
        "# So how would you fix that? Like any other program...you have callbacks! Let's see them in action.\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoh, logs={}):\n",
        "    if(logs.get('accuracy')>0.9):\n",
        "      print(\"\\Reached 90% accuracy so cacelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels),  (test_images, test_labels) = mnist.load_data()\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n",
        "\n"
      ],
      "metadata": {
        "id": "eTHwOyDseQ0x",
        "outputId": "94553110-0fd1-4806-a98b-6790739e7ece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4767 - accuracy: 0.8294\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3574 - accuracy: 0.8697\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 10s 6ms/step - loss: 0.3217 - accuracy: 0.8809\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2967 - accuracy: 0.8908\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2780 - accuracy: 0.8961\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ee0e9303a0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83q5VUimyzcY"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}